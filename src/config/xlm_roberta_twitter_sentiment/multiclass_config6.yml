pretrained_model: "cardiffnlp/twitter-xlm-roberta-base-sentiment"
lang: tam_mal_shuffled
run_id: xlm_roberta_sentiment
n_splits: 5
use_gpu: True
disable_loading_bar: True
TrainingArguments:
  # output_dir: "../outputs/tam/" # base directory path; concatenated with run_id and fold_id
  per_device_train_batch_size: 75
  per_device_eval_batch_size: 75
  evaluation_strategy: epoch
  log_level: debug
  logging_strategy: epoch
  # logging_first_step: True
  # evaluation_strategy: steps
  # log_level: debug
  # logging_steps: 1
  # logging_strategy: steps
  # logging_first_step: True
  save_strategy: steps
  # save_strategy: epoch
  seed: !!python/name:__main__.GLOBAL_SEED
  load_best_model_at_end: False
  resume_from_checkpoint: False
  disable_tqdm: True
  # push_to_hub: False
phases:
  - TrainingArguments:
      learning_rate: 2.0e-5
      num_train_epochs: 10
      #- weight_decay: 0.01
      save_steps: 5
    FinetuningArguments:
      num_layers_to_freeze: 12
  - TrainingArguments:
      learning_rate: 1.0e-6
      num_train_epochs: 10
      save_steps: 5
    FinetuningArguments:
      num_layers_to_freeze: 11
  - TrainingArguments:
      learning_rate: 1.0e-7
      num_train_epochs: 10
      weight_decay: 0.01
      save_steps: 5
    FinetuningArguments:
      num_layers_to_freeze: 10
  - TrainingArguments:
      learning_rate: 4.0e-5
      num_train_epochs: 25
      #- weight_decay: 0.01
      save_steps: 5
    FinetuningArguments:
      num_layers_to_freeze: 12
  - TrainingArguments:
      learning_rate: 1.0e-6
      num_train_epochs: 15
      weight_decay: 0.01
      save_strategy: "no"
    FinetuningArguments:
      num_layers_to_freeze: 9
  - TrainingArguments:
      learning_rate: 2.0e-6
      num_train_epochs: 15
      weight_decay: 0.01
      save_strategy: "no"
    FinetuningArguments:
      num_layers_to_freeze: 8
  - TrainingArguments:
      learning_rate: 4.0e-5
      num_train_epochs: 25
      weight_decay: 0.01
      save_steps: 10
    FinetuningArguments:
      num_layers_to_freeze: 12
  - TrainingArguments:
      learning_rate: 1.0e-7
      num_train_epochs: 20
      #weight_decay: 0.01
      save_steps: 20
    FinetuningArguments:
      num_layers_to_freeze: 7
  - TrainingArguments:
      learning_rate: 5.0e-8
      num_train_epochs: 200
      #weight_decay: 0.01
      save_steps: 50
    FinetuningArguments:
      num_layers_to_freeze: 2
  - TrainingArguments:
      learning_rate: 1.0e-6
      num_train_epochs: 10
      #weight_decay: 0.01
      save_strategy: "no"
    FinetuningArguments:
      num_layers_to_freeze: 12
